{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnnuoKReNeUx",
        "outputId": "50a8b02f-5781-471a-ecd9-7616d419a456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "<ipython-input-10-a3f4c3a0a557>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(batch['label']).unsqueeze(1).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "Predicted difficulty for the test text: 0.6074645519256592\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "df = pd.read_csv('/content/Untitled spreadsheet - Sheet1 (2).csv')\n",
        "# Sample dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {'text': self.texts[idx], 'label': self.labels[idx]}\n",
        "\n",
        "# Tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
        "\n",
        "# Sample data\n",
        "texts = df[\"Questions\"]\n",
        "\n",
        "labels = df[\"Difficulty\"]  # Replace with your actual difficulty scores\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = CustomDataset(texts, labels)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Training parameters\n",
        "lr = 1e-5\n",
        "epochs = 5\n",
        "\n",
        "# Optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "i=0\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    print(i)\n",
        "    i=i+1\n",
        "    for batch in dataloader:\n",
        "        inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True)\n",
        "        labels = torch.tensor(batch['label']).unsqueeze(1).float()\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        predictions = outputs.logits\n",
        "        loss = loss_fn(predictions, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Testing the model\n",
        "with torch.no_grad():\n",
        "    test_text = \"What is the significance of the papyrus plant in ancient Egyptian civilization?\"\n",
        "    test_input = tokenizer(test_text, return_tensors='pt', padding=True, truncation=True)\n",
        "    test_output = model(**test_input).logits.item()\n",
        "    print(f'Predicted difficulty for the test text: {test_output}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    test_text = \"If the economic practices of the Indus Valley Civilization were still being used today, what will be the consequences?\"\n",
        "    test_input = tokenizer(test_text, return_tensors='pt', padding=True, truncation=True)\n",
        "    test_output = model(**test_input).logits.item()\n",
        "    print(f'Predicted difficulty for the test text: {test_output}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcMeGGC2ZxEN",
        "outputId": "d2c7e71f-3308-443b-adfa-10bb0e20686a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted difficulty for the test text: 0.8284471035003662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Save the model\n",
        "model_path = \"history_difficulty_model.pth\"\n",
        "# Save the model architecture\n",
        "torch.save(model, 'history_difficulty_model_architecture.pth')\n",
        "\n",
        "\n",
        "# Load the model\n",
        "loaded_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
        "loaded_model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "loaded_model.eval()\n",
        "\n",
        "# Now, you can use the loaded_model for inference\n",
        "with torch.no_grad():\n",
        "    test_text = \"What is the significance of the papyrus plant in ancient Egyptian civilization?\"\n",
        "    test_input = tokenizer(test_text, return_tensors='pt', padding=True, truncation=True)\n",
        "    test_output = loaded_model(**test_input).logits.item()\n",
        "    print(f'Predicted difficulty for the test text: {test_output}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyonDVYCmQPo",
        "outputId": "16fe3ba0-ce5e-45f2-ec57-a621a7dfa6d5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted difficulty for the test text: 0.6074645519256592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the model state dictionary file\n",
        "files.download('history_difficulty_model.pth')\n",
        "\n",
        "# Download the model architecture file\n",
        "files.download(\"history_difficulty_model_architecture.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "wequKlTgmkpE",
        "outputId": "0c933a63-d1be-46d5-e22e-3ace7f98176f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8bc300fa-9028-4a28-9cc9-2c5e9f5385c0\", \"history_difficulty_model.pth\", 438014397)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8291971e-df2f-48fa-967e-d8532e92d685\", \"history_difficulty_model_architecture.pth\", 438063262)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    test_text = \"State life in the Middle Ages\"\n",
        "    test_input = tokenizer(test_text, return_tensors='pt', padding=True, truncation=True)\n",
        "    test_output = loaded_model(**test_input).logits.item()\n",
        "    print(f'Predicted difficulty for the test text: {test_output}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVGSwz12pEAU",
        "outputId": "ce09fcd5-1eea-419c-c6d6-a066dfec9b93"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted difficulty for the test text: 0.2960759699344635\n"
          ]
        }
      ]
    }
  ]
}